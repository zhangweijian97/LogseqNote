---
title: 数据挖掘与探索公式梳理
published: true
permalink: %E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E4%B8%8E%E6%8E%A2%E7%B4%A2%E5%85%AC%E5%BC%8F%E6%A2%B3%E7%90%86
---

- 1 First Steps in Exploratory Data Analysis
		- 1.1 Numerical Data Description
			- 1.1.1 Location
				- mean，公式略
					- 积分版
					- 向量版
				- median，公式略
				- 公式1.7是上面两个的混合
				- mode
			- 1.1.2 Scale
				- 方差，公式略
					- 积分版
				- MAD，公式1.10
				- IQR，Q3-Q1
			- 1.1.3 Shape
				- skewness，1.13
				- Galton’s measure of skewness，1.14
				- kurtosis，1.15
				- robust kurtosis，1.16
			- 1.1.4 Multivariate Measures
				- 协方差，1.17和1.18
				- Pearson’s correlation coefficient，1.19
				- 标准差
				- sample covariance matrix，1.28
					-
					  id:: 60364900-cd28-4b19-87e1-40d3a4549ea9
					  $$\boldsymbol{\Sigma}=\frac{1}{n}\boldsymbol{X}\boldsymbol{X}^\top$$
					- eigenvalue decomposition
					  id:: 603648e7-1114-436c-8b9e-258da79f3fc9
						-
						  $$\operatorname{Cov}[\boldsymbol{x}]  = \boldsymbol{\Sigma} = \boldsymbol{U}\boldsymbol{\Lambda} \boldsymbol{U}^\top$$
					- trace, 1.43
					- 性质，做线性变化之后
						-
						  $$
						  \begin{aligned}
						  \operatorname{Cov}[\boldsymbol{A} \boldsymbol{x}+\boldsymbol{b}] &=\boldsymbol{A} \operatorname{Cov}[\boldsymbol{x}] \boldsymbol{A}^{\top}
						  \end{aligned}
						  $$
				- sample correlation matrix，1.41
					-
					  $$
					  \rho(\boldsymbol{x})=\operatorname{diag}\left(\frac{1}{\operatorname{std}(\boldsymbol{x})}\right) \operatorname{cov}(\boldsymbol{x}) \operatorname{diag}\left(\frac{1}{\operatorname{std}(\boldsymbol{x})}\right)
					  $$
				- non linear relationship，1.42
				- Kendall’s $\tau$，1.43
					-
					  $$
					  \tau(x, y)=\frac{n_{c}(x, y)-n_{d}(x, y)}{n(n-1) / 2}
					  $$
		- 1.2 Data Visualisation
			- 这节看lab，研究一下参数变化
			- 1.2.1 Bar Plot
			- 1.2.2 Box Plot
			- 1.2.3 Scatter Plot
			- 1.2.4 Histogram
			- 1.2.5 Kernel Density Plot
				- kernel density estimate，1.47
			- 1.2.6 Violin Plot
		- 1.3 Data Pre-Processing
			- 1.3.1 Standardisation
				- 注意这一节中$x$表示没有中心化的，$\tilde x$表示中心化的，和PCA那边的不一样
				- certring matrix
				  id:: 60364b4a-32fb-4ab7-997b-fb38423e1061
					-
					  $$
					  \boldsymbol{C}_{n}=\boldsymbol{I}_{n}-\frac{1}{n} \mathbf{1}_{n} \mathbf{1}_{n}^{\top}
					  $$
					-
					  $$\boldsymbol{X}=\tilde\boldsymbol{X}\boldsymbol{C}_n$$
					- 左列右行
				- sample covarice matrix with certring matrix，1.66
					-
					  $$\boldsymbol{\Sigma}=\frac{1}{n}\boldsymbol{X}\boldsymbol{C}_n\boldsymbol{X}^\top$$
			- 1.3.2 Outlier Detection and Removal
				- Tukey’s fences，1.68
					-
					  $$[Q_1-k\text{IQR(x)}, Q_3+k\text{IQR(x)}]$$
					- common $k=1.5$
	- 2 Principal Component Analysis
		- 2.1 PCA by Variance Maximisation
			- 2.1.1 First Principal Component Direction
				- optimisation problem, 2.2
					-
					  $$\begin{array}{cl}
					  \underset{w_{1}}{\operatorname{maximise}} & w_{1}^{\top} \mathbf{\Sigma} w_{1} \\
					  \text { subject to } & \left\|w_{1}\right\|=1
					  \end{array}$$
					- 解法在2.3到2.7
				- Principal Component Scores
					-
					  $$z_i = \boldsymbol{w}_i^\top\boldsymbol{x}$$
					- 上面这个是原公式，下面是我自己写的
					-
					  $$z_{ij} = \boldsymbol{w}_i^\top\boldsymbol{x}_j$$
					-
					  $$\boldsymbol{z}_i^\top = \boldsymbol{w}_i^\top\boldsymbol{X}$$
					-
					  id:: 6036c287-de39-431b-9627-474b3ce79e80
					  $$\boldsymbol{z}_j = \boldsymbol{U}_k^\top\boldsymbol{x}_j$$
					-
					  id:: 6036c30d-feb4-4f88-87c3-c257a5f75363
					  $$\boldsymbol{Z} = \boldsymbol{U}_k^\top\boldsymbol{X}$$
						- Z 的维度，k $\times$ n
			- 2.1.2 Subsequent Principal Component Directions，
				- 证明主成分是不相关的 The principal components are uncorrelated 2.16到2.22
				- m-th principal component direction 2.23
					-
					  $$\begin{array}{ll}
					  \underset{\boldsymbol{w}_{m}}{\operatorname{maximise}} & \boldsymbol{w}_{m}^{\top} \boldsymbol{\Sigma} \boldsymbol{w}_{m} \\
					  \text { subject to } & \left\|\boldsymbol{w}_{m}\right\|=1 \\
					  & \boldsymbol{w}_{m}^{\top} \boldsymbol{w}_{i}=0 \quad i=1, \ldots, m-1
					  \end{array}$$
					- variance explained，2.24
						-
						  $$\sum_{m=1}^k\lambda_m$$
					- fraction of variance explained,2.26
						-
						  $$\cfrac{\sum_{m=1}^k\lambda_m}{\sum_{m=1}^d\lambda_m}$$
			- 2.1.3 Simultaneous Variance Maximisation
				- 公式 2.27
		- 2.2 PCA by Minimisation of Approximation Error
			- 下面的W也是U
			- Projection Matrix，公式2.28
				-
				  $$
				  \boldsymbol{P}=\sum_{i=1}^{k} \boldsymbol{w}_{i} \boldsymbol{w}_{i}^{\top}=\boldsymbol{W}_{k} \boldsymbol{W}_{k}^{\top}, \quad \boldsymbol{W}_{k}=\left(\boldsymbol{w}_{1}, \ldots, \boldsymbol{w}_{k}\right)
				  $$
			- 投影的x 公式2.30，$\hat x = Px = \sum_{i=1}^{k} \boldsymbol{w}_{i} \boldsymbol{w}_{i}^{\top}x =\boldsymbol{W}_{k} \boldsymbol{W}_{k}^{\top}x = \sum_{i=1}^k\boldsymbol{w}_{i}z_i$
			  id:: 6036bb2f-90a3-4763-8761-b3f704307e42
			- 优化问题，公式2.29
			- smallest expected approximation error，公式2.36
				-
				  $$\sum_{i=k+1}^d\lambda_i$$
		- 2.3 PCA by Low Rank Matrix Approximation
			- 2.3.1 Approximating the Data Matrix
				- singular value decomposition, 公式2.39
					-
					  $$\boldsymbol{X} = \boldsymbol{U}\boldsymbol{S}\boldsymbol{V}^\top$$
					- U, $d \times d$; S, $d \times n$; V, $n \times n$; X, $d \times n$
					- X又可以写作
						-
						  $$\boldsymbol{X}=\sum_{i=1}^r s_i \boldsymbol{u}_i\boldsymbol{v}_i^\top$$
				- optimisation problem，公式2.42
					-
					  $$
					  \begin{array}{ll}
					  \underset{M}{\operatorname{minimise}} & \sum_{i j}\left((\boldsymbol{X})_{i j}-(\boldsymbol{M})_{i j}\right)^{2} \\
					  \text { subject to } & \operatorname{rank}(\boldsymbol{M})=k
					  \end{array}
					  $$
					- Frobenius norm
				- 公式2.43
					-
					  $$\hat\boldsymbol{X}=\sum_{i=1}^k s_i \boldsymbol{u}_i\boldsymbol{v}_i^\top$$
				- approximation error，公式2.44
					-
					  $$
					  \|\boldsymbol{X}-\hat{\boldsymbol{X}}\|_{F}=\sum_{i=k+1}^{r} s_{i}^{2}
					  $$
				- relate to principal component analysis
					- $\boldsymbol{u}_i$ 是 principal component directions
					- $s_i$ 和 $\lambda_i$ 公式2.45
						-
						  $$\lambda_i=\cfrac{s_i^2}{n}$$
					- principal component scores (**after scaling?**)，公式2.46
						-
						  $$\boldsymbol{z}_i^\top=\boldsymbol{u}_i^\top\boldsymbol{X}=s_i\boldsymbol{v}_i^\top$$
					- 公式2.47
						-
						  $$\hat\boldsymbol{X}=\sum_{i=1}^k \boldsymbol{u}_i\boldsymbol{z}_i^\top$$
					- 证明在公式2.48 到 2.50（这里似乎有一道旧卷题考到？）
			- 2.3.2 Approximating the Sample Covariance Matrix
				- optimisation problem 2.51
					-
					  $$
					  \begin{array}{cl}
					  \underset{\boldsymbol{M}}{\operatorname{minimise}} & \|\boldsymbol{\Sigma}-\boldsymbol{M}\|_{F} \\
					  \text { subject to } & \operatorname{rank}(\boldsymbol{M})=k \\
					  & \boldsymbol{M}^{\top}=\boldsymbol{M}
					  \end{array}
					  $$
			- 2.3.3 Approximating the Gram Matrix
				- Gram matrix 2.52
					-
					  id:: 6036c587-f200-48ec-9866-3189543f2c77
					  $$\boldsymbol{G} = \boldsymbol{X}^\top\boldsymbol{X}$$
						- n $\times$ n matrix
					- 用SVD得到G的EVD, 公式2.53
					  id:: 6036c5b3-1d6f-478d-8cc6-90251b30da71
						-
						  $$\boldsymbol{G} = \boldsymbol{V} \tilde\boldsymbol{\Lambda}\boldsymbol{V}^\top$$
						-
						  $$\tilde\boldsymbol{\Lambda}=\boldsymbol{S}^\top\boldsymbol{S}$$
				- best rank k approximation of the Gram matrix 公式2.54
					-
					  $$\hat\boldsymbol{G}=\sum_{i=1}^k \boldsymbol{v}_i s_i^2\boldsymbol{v}_i^\top = \sum_{i=1}^k \boldsymbol{z}_i \boldsymbol{z}_i^\top$$
				- principal component scores, 公式2.57
				  id:: 6036c688-04a1-4c3b-ae7f-6293217ca49e
					-
					  $$
					  \boldsymbol{Z}=\sqrt{\tilde{\boldsymbol{\Lambda}}_{k}} \boldsymbol{V}_{k}^{\top}
					  $$
					- k $\times$ n matrix
		- 2.4 Probabilistic PCA
			- 2.4.1 Probabilistic Model
				- 随机变量 $\boldsymbol{z}$ 统计独立，标准正太分布 2.58
					-
					  $$
					  p(\boldsymbol{z})=\mathcal{N}(\boldsymbol{z} \mid \mathbf{0}, \boldsymbol{I})
					  $$
				- 正太分布 2.59
					-
					  $$
					  \mathcal{N}(\boldsymbol{x} \mid \boldsymbol{\mu}, \mathbf{\Sigma})=\frac{1}{\sqrt{(2 \pi)^{d}|\operatorname{det}(\mathbf{\Sigma})|}} \exp \left(-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^{\top} \boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu})\right)
					  $$
				- 数据 $\boldsymbol{x}$ 2.60
					-
					  $$\boldsymbol{x} = \boldsymbol{W}\boldsymbol{z}+\boldsymbol{\mu}+\boldsymbol{\epsilon}$$
					- W, $d \times k$
					- $\mu$, $d$-dimension
					- $\epsilon$, $d$-dimension zero-mean Gaussian-distributed noise **variable**
			- 2.4.2 Joint, Conditional and Observation Distributions
				- 条件概率分布 公式2.61
					-
					  $$
					  p(\boldsymbol{x}|\boldsymbol{z})=\mathcal{N}(\boldsymbol{x} \mid \boldsymbol{W}\boldsymbol{z}+\boldsymbol{\mu}, \sigma^2\boldsymbol{I})
					  $$
				- 联合概率分布 公式2.62
					-
					  $$
					  p(\boldsymbol{z}, \boldsymbol{x})=\frac{1}{\text { const }} \exp \left(-\frac{1}{2}\left[(\boldsymbol{x}-\boldsymbol{W} \boldsymbol{z}-\boldsymbol{\mu})^{\top}\left(\frac{1}{\sigma^{2}} \boldsymbol{I}\right)(\boldsymbol{x}-\boldsymbol{W} \boldsymbol{z}-\boldsymbol{\mu})+\boldsymbol{z}^{\top} \boldsymbol{z}\right]\right)
					  $$
					- 协方差矩阵 covariance 公式2.68
						-
						  $$
						  \operatorname{Cov}\left[\left(\begin{array}{l}
						  \boldsymbol{z} \\
						  \boldsymbol{x}
						  \end{array}\right)\right]=\left(\begin{array}{cc}
						  \boldsymbol{I}+\boldsymbol{W}^{\top} \frac{1}{\sigma^{2}} \boldsymbol{W} & -\boldsymbol{W}^{\top} \frac{1}{\sigma^{2}} \\
						  -\frac{1}{\sigma^{2}} \boldsymbol{W} & \frac{1}{\sigma^{2}} \boldsymbol{I}
						  \end{array}\right)^{-1}=\left(\begin{array}{cc}
						  \boldsymbol{I} & \boldsymbol{W}^{\top} \\
						  \boldsymbol{W} & \boldsymbol{W} \boldsymbol{W}^{\top}+\sigma^{2} \boldsymbol{I}
						  \end{array}\right)
						  $$
					- 平均值 mean 公式2.70
						-
						  $$
						  \mathbb{E}\left[\left(\begin{array}{l}
						  \boldsymbol{z} \\
						  \boldsymbol{x}
						  \end{array}\right)\right]=\left(\begin{array}{cc}
						  \boldsymbol{I} & \boldsymbol{W}^{\top} \\
						  \boldsymbol{W} & \sigma^{2} \boldsymbol{I}+\boldsymbol{W} \boldsymbol{W}^{\top}
						  \end{array}\right)\left(\begin{array}{c}
						  -\boldsymbol{W}^{\top} \frac{1}{\sigma^{2}} \boldsymbol{\mu} \\
						  \frac{1}{\sigma^{2}} \boldsymbol{\mu}
						  \end{array}\right)=\left(\begin{array}{l}
						  \mathbf{0} \\
						  \boldsymbol{\mu}
						  \end{array}\right)
						  $$
				- 边缘概率分布 公式 2.71
					-
					  $$
					  p(\boldsymbol{x})=\mathcal{N}\left(\boldsymbol{x} \mid \boldsymbol{\mu}, \boldsymbol{W} \boldsymbol{W}^{\top}+\sigma^{2} \boldsymbol{I}\right)
					  $$
			- 2.4.3 Maximum Likelihood
				- log-likelihood 公式 2.73 到 2.75
					-
					  $$
					  \begin{aligned}
					  \log p\left(\boldsymbol{X} \mid \boldsymbol{W}, \sigma^{2}\right) &=\sum_{i=1}^{n} \log p\left(\boldsymbol{x}_{i} \mid \boldsymbol{\mu}, \boldsymbol{W}, \sigma^{2}\right) \\
					  &=\sum_{i=1}^{n} \log \mathcal{N}\left(\boldsymbol{x}_{i} \mid \boldsymbol{\mu}, \mathbf{\Sigma}\right)
					  \end{aligned}
					  $$
					  with $\boldsymbol{\mu}=\mathbf{0}$ and
					  $$
					  \boldsymbol{\Sigma}=\boldsymbol{W} \boldsymbol{W}^{\top}+\sigma^{2} \boldsymbol{I}
					  $$
				- 利用公式2.78 到 2.80，继续推导上式 2.81 到 2.84
					-
					  $$
					  \begin{aligned}
					  \log p\left(\boldsymbol{X} \mid \boldsymbol{W}, \sigma^{2}\right) &=\sum_{i=1}^{n} \log \mathcal{N}\left(\boldsymbol{x}_{i} \mid \mathbf{0}, \boldsymbol{\Sigma}\right) \\
					  & \stackrel{(2.59)}{=} \sum_{i=1}^{n}\left[-\frac{1}{2} d \log (2 \pi)-\frac{1}{2} \log (|\operatorname{det}(\boldsymbol{\Sigma})|)-\frac{1}{2} \boldsymbol{x}_{i}^{\top} \boldsymbol{\Sigma}^{-1} \boldsymbol{x}_{i}\right] \\
					  &=-\frac{n}{2}[d \log (2 \pi)+\log (|\operatorname{det}(\boldsymbol{\Sigma})|)]-\frac{1}{2} \sum_{i=1}^{n} \boldsymbol{x}_{i}^{\top} \boldsymbol{\Sigma}^{-1} \boldsymbol{x}_{i} \\
					  & \stackrel{(2.79)}{=}-\frac{n}{2}\left[d \log (2 \pi)+\log (|\operatorname{det}(\boldsymbol{\Sigma})|)+\operatorname{trace}\left(\boldsymbol{\Sigma}^{-1} \hat{\boldsymbol{\Sigma}}\right)\right]
					  \end{aligned}
					  $$
					-
					  $$\hat\boldsymbol{\Sigma}=\frac{1}{n}\boldsymbol{X}\boldsymbol{X}^\top$$
				- 最大化上式，解得 公式 2.85 和 公式 2.86
					-
					  $$
					  \boldsymbol{W}_{\mathrm{ML}}=\boldsymbol{U}_{k}\left(\boldsymbol{\Lambda}_{k}-\sigma^{2} \boldsymbol{I}\right)^{1 / 2} \boldsymbol{R}
					  $$
						- 对$\hat\boldsymbol{\Sigma}$ 求EVD，得到 $\boldsymbol{U}_{k}$ 和 $\boldsymbol{\Lambda}_{k}$
						- R不唯一，可以取I
					-
					  $$
					  \sigma_{\mathrm{ML}}^{2}=\frac{1}{d-k} \sum_{i=k+1}^{d} \lambda_{i}
					  $$
						- represents the average lost variance per residual dimension
			- 2.4.4 Relation to PCA
				- PCA计算 principal component scores
					- {{embed ((6036c287-de39-431b-9627-474b3ce79e80))}}
				- PPCA
					- 公式2.87是 p(z|x) 的协方差
					- 公式2.90是 p(z|x) 的均值
					- 公式2.91是 p(z|x) 的分布，可以用x来计算z
				- 上面的方法需要知道W
				- PCA 计算 x 的 projection
					- {{embed ((6036bb2f-90a3-4763-8761-b3f704307e42))}}
					- 公式2.93到2.99略，证明了PCA can be seen as a special case of probabilistic PCA when the noise variance σ2 is negligibly small
	- 3 Dimensionality Reduction
		- 3.1 Linear Dimensionality Reduction
			- 3.1.1 From Data Points
				- 已知未中心化的 $\tilde\boldsymbol{X}$
				- 先中心化
					- {{embed ((60364b4a-32fb-4ab7-997b-fb38423e1061))}}
				- 然后计算协方差
					- {{embed ((60364900-cd28-4b19-87e1-40d3a4549ea9))}}
				- 做EVD得到Uk
					- {{embed ((603648e7-1114-436c-8b9e-258da79f3fc9))}}
				- 计算scores
					- {{embed ((6036c30d-feb4-4f88-87c3-c257a5f75363))}}
				- 或者计算G
					- {{embed ((6036c587-f200-48ec-9866-3189543f2c77))}}
				- 做SVD的到A，V
					- {{embed ((6036c5b3-1d6f-478d-8cc6-90251b30da71))}}
				- 计算scores
					- {{embed ((((6036c688-04a1-4c3b-ae7f-6293217ca49e))))}}
			- 3.1.2 From Inner Products
			  id:: 6036458f-8df4-428c-b2b3-7ba60689e716
				- 已知未中心化的 $\tilde\boldsymbol{G}$
				- 先中心化，公式3.10
					-
					  $$\boldsymbol{G} = \boldsymbol{C}_n\tilde\boldsymbol{G}\boldsymbol{C}_n$$
				- 重复前面
			- 3.1.3 From Distances
			  id:: 6036458f-c6f3-4d1d-b854-a86ee25e7d47
				- 已知未中心化的数据，之间的距离 $\delta_{ij}^2$, 或表示为$\boldsymbol{\Delta}$
				- 公式3.12到公式3.21一通推导，得到公式3.22，用$\boldsymbol{\Delta}$ 计算 G
					-
					  $$\boldsymbol{G} = -\cfrac{1}{2} \boldsymbol{C}_n\boldsymbol{\Delta}\boldsymbol{C}_n$$
				- 重复前面
			- 3.1.4 Example
				- 无公式，跳过
		- 3.2 Dimensionality Reduction by Kernel PCA
			- 3.2.1 Idea
				- 把x的feature混合得到更高dimension的特征，公式3.23
					-
					  $$
					  \phi(\boldsymbol{x})=\left(x_{1}, \cdots, x_{d}, x_{1} x_{2}, \cdots, x_{1} x_{d}, \cdots, x_{d} x_{d}\right)^{\top}
					  $$
				- data matrix 公式3.24
					-
					  $$\Phi = (\phi_1, \cdots, \phi_n)$$
			- 3.2.2 Kernel Trick
				- 公式 3.26
					-
					  $$
					  (\tilde{\boldsymbol{G}})_{i j}=\boldsymbol{\phi}_{i}^{\top} \boldsymbol{\phi}_{j}=\phi\left(\boldsymbol{x}_{i}\right)^{\top} \phi\left(\boldsymbol{x}_{j}\right)=k\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right)
					  $$
					- can be used to compute the (**uncentred**) Gram matrix of $\Phi$
				- 公式3.27是两个k的举例
				- 既然得到未中心化的$\tilde{\boldsymbol{G}}$，可以用 ((6036458f-8df4-428c-b2b3-7ba60689e716)) 的方法来计算Scores
			- 3.2.3 Example
				- 略
		- 3.3 Multidimensional Scaling
			- 3.3.1 Metric MDS
				- 已知不相似矩阵
				- 优化问题 公式3.31
					-
					  $$
					  \operatorname{minimise}_{z_{1}, \ldots, z_{n}} \sum_{i<j} w_{i j}\left(\left\|z_{i}-z_{j}\right\|-\delta_{i j}\right)^{2}
					  $$
				- solution，Sammon nonlinear mapping
					-
					  $$
					  w_{i j}=1 / \delta_{i j}
					  $$
			- 3.3.2 Nonmetric MDS
				- 不相似矩阵未知，但是rank函数已知
				- 优化问题 公式3.33
					-
					  $$
					  \underset{z_{1}, \ldots, z_{n}, f}{\operatorname{minimise}} \sum_{i<i} w_{i j}\left(\left\|z_{i}-z_{j}\right\|-f\left(\delta_{i j}\right)\right)^{2}
					  $$
				- 用回归的方法解，课本只给了论文链接 Izenman, 2008
					- Modern Multivariate Statistical Techniques: Regression, Classiﬁcation, and Manifold Learning
			- 3.3.3 Classical MDS
				- 数据的维数未知，数值也未知
				- 已知不相似矩阵，用 ((6036458f-c6f3-4d1d-b854-a86ee25e7d47)) 的方法可以计算Z
				- 优化问题 公式 3.36
					-
					  $$
					  \begin{array}{ll}
					  \underset{M}{\operatorname{minimise}} & \left\|\left(-\frac{1}{2} \boldsymbol{C}_{n} \boldsymbol{\Delta} \boldsymbol{C}_{n}\right)-\boldsymbol{M}^{\top} \boldsymbol{M}\right\|_{F} \\
					  \text { subject to } & \operatorname{rank}\left(\boldsymbol{M}^{\top} \boldsymbol{M}\right)=k
					  \end{array}
					  $$
				- 解就是Z
				- M来自Frobenius norm，就别管是啥了
			- 3.3.4 Example
				- 略
		- 3.4 Isomap
			- 没公式，略
			- 就是把classical MDS的距离用geodesic距离表示
			- 用m-nearest neighbours算法计算距离
		- 3.5 UMAP
			- 类似Isomap，分割加权图，权重的计算公式 公式3.37
				-
				  $$
				  w_{i}^{(\boldsymbol{Y})}\left(\boldsymbol{y}_{i}, \boldsymbol{y}_{j}\right)=\exp \left(-\frac{\left\|\boldsymbol{y}_{i}-\boldsymbol{y}_{j}\right\|-\rho_{i}}{\sigma_{i}}\right)
				  $$
					- where ρi denotes the distance to the nearest neighbour and σi is a measure of the size of the neightbourhood around yi
			- To symmetrise，重新设置为 公式3.38
				-
				  $$
				  w^{(\boldsymbol{Y})}\left(\boldsymbol{y}_{i}, \boldsymbol{y}_{j}\right)=w_{i}^{(\boldsymbol{Y})}\left(\boldsymbol{y}_{i}, \boldsymbol{y}_{j}\right)+w_{j}^{(\boldsymbol{Y})}\left(\boldsymbol{y}_{j}, \boldsymbol{y}_{i}\right)-w_{i}^{(\boldsymbol{Y})}\left(\boldsymbol{y}_{i}, \boldsymbol{y}_{j}\right) w_{j}^{(\boldsymbol{Y})}\left(\boldsymbol{y}_{j}, \boldsymbol{y}_{i}\right)
				  $$
				- 得到权重矩阵W
			- 已知观察数据 $\tilde{\boldsymbol{X}}$ 和 lower dimensional representation $\boldsymbol{M}$
			- 计算交叉熵 cross entropy 公式3.39
				-
				  $$
				  C\left(\boldsymbol{W}^{(\tilde{\boldsymbol{X}})}, \boldsymbol{W}^{(M)}\right)=\sum_{i<j} w_{i j}^{(\tilde{\boldsymbol{X}})} \log \left(\frac{w_{i j}^{(\tilde{\boldsymbol{X}})}}{w_{i j}^{(M)}}\right)+\left(1-w_{i j}^{(\tilde{\boldsymbol{X}})}\right) \log \left(\frac{\left(1-w_{i j}^{(\tilde{\boldsymbol{X}})}\right)}{\left(1-w_{i j}^{(\boldsymbol{M})}\right)}\right)
				  $$
			- 优化函数
				-
				  $$
				  \begin{array}{ll}
				  \underset{M}{\operatorname{minimise}} & C\left(\boldsymbol{W}^{(\tilde{\boldsymbol{X}})}, \boldsymbol{W}^{(M)}\right) 
				  \end{array}
				  $$
			- 求解得到 lower dimensional representation Z
			- 解法在 McInnes, Healy, and Melville, 2018, Sections 2, 3
				- Umap: Uniform manifold approximation and projection for dimension reduction
	- 4 Predictive Modelling and Generalisation
		- 4.1 Prediction and Training Loss
			- 4.1.1 Prediction Loss
				- predictor $$\boldsymbol{x}$$
				- target $$y$$
				- 不论是回归任务或者分类任务
				- 理想目标，得到 条件概率分布 $$p(y|\boldsymbol{x})$$
				- 但理想是丰满的，现实是骨感的
				- 只能寻找到一个预测函数 $$h(\boldsymbol{x})$$
				- 预测函数的输出是我们对目标 $$y$$ 的估计 $$\hat y=h(\boldsymbol{x})$$
				- 目标的真实值和我们的预测值之间存在一个损失loss函数 $$\mathcal{L}(\hat y, y)$$
				- prediction loss 预测损失，是损失的期望 公式 4.1
					-
					  $$
					  \mathcal{J}(h)=\mathbb{E}_{\hat{y}, y}[\mathcal{L}(\hat{y}, y)]=\mathbb{E}_{\boldsymbol{x}, y}[\mathcal{L}(h(\boldsymbol{x}), y)]
					  $$
				- 模型的优化问题是最小化期望损失 公式 4.2
					-
					  $$
					  \begin{array}{ll}
					  \underset{h}{\operatorname{minimise}} & \mathcal{J}(h) 
					  \end{array}
					  $$
				- （画外音：所以预测损失是指在整个数据集上算出来的，下面的训练损失就是指在训练集上算出来的）
				  id:: 6037059e-7f8b-4e5f-99c7-3d4a1e3c591c
			- 4.1.2 Training Loss
				- 但期望没法直接计算，所以就近似为 公式 4.4
					-
					  $$
					  \mathcal{J}(h)\approx \cfrac{1}{n} \sum_{i=1}^n\mathcal{L}(h(\boldsymbol{x}_i), y_i)
					  $$
				- 训练集定义为 $$\mathcal{D}^{\text{train}}$$
				- 重新定义预测函数为 $$h(\boldsymbol{x}) = h_{\boldsymbol{\lambda}}(\boldsymbol{x};\boldsymbol{\theta})$$
					- $\boldsymbol{\theta}$ 是函数内参数
					- $\boldsymbol{\lambda}$ 是超参数
				- 上面的损失函数 $$\mathcal{L}$$ 也算不了，用一个 proxy loss function 代理损失函数 $$L$$  替代
				- training loss 训练损失 $$J_{\boldsymbol{\lambda}}(h)$$，是预测损失 $\mathcal{J}(h)$ 的代理 公式 4.6
					-
					  $$
					  J_{\boldsymbol{\lambda}}(h)= \cfrac{1}{n} \sum_{i=1}^n L(h_{\boldsymbol{\lambda}}(\boldsymbol{x}_i;\boldsymbol{\theta}), y_i)
					  $$
				- 固定住超参数$\boldsymbol{\lambda}$，最小化训练损失，得到最优参数 $\hat\boldsymbol{\theta}_{\boldsymbol{\lambda}}$ 和最优预测函数 $\hat\boldsymbol{h}_{\boldsymbol{\lambda}}(\boldsymbol{x})$ 公式 4.7
					-
					  id:: 60370999-8dd7-4c7d-b72c-f935dcfe772e
					  $$
					  \hat{h}_{\boldsymbol{\lambda}}(\boldsymbol{x})=h_{\boldsymbol{\lambda}}\left(\boldsymbol{x} ; \hat{\boldsymbol{\theta}}_{\boldsymbol{\lambda}}\right), \quad \hat{\boldsymbol{\theta}}_{\boldsymbol{\lambda}}=\underset{\boldsymbol{\theta}}{\operatorname{argmin}} J_{\boldsymbol{\lambda}}(\boldsymbol{\theta})
					  $$
				- 不能选产生最小训练损失的超参数，因为过拟合训练集，没有足够的泛化性能
			- 4.1.3 Example
				- 做题推导可以参考这里
		- 4.2 Generalisation Performance
			- 4.2.1 Generalisation for Prediction Functions and Algorithms
				- 说一个模型具有好的泛化性能，就是说对于最优预测函数 $\hat{h}$ ，它的预测损失 $\mathcal{J}(\hat h)$ 很小 公式 4.21
					-
					  $$
					  \mathcal{J}(\hat h)=\mathbb{E}_{\boldsymbol{x}, y}[\mathcal{L}(\hat h(\boldsymbol{x}), y)]]
					  $$
					- measures the performance of a specific $\hat{h}$
				- 预测损失 $\mathcal{J}(\hat h)$ 又称作泛化损失 generalisation loss 或 测试损失 test loss
				- 但上面这玩意儿也是不能算的，因为并不知道数据的真实分布
				- 用 ((603645b8-d154-41bf-b0e3-3c0bd80d3cb2)) 的方法 held-out data 来估计损失 $\mathcal{L}(\hat h(\boldsymbol{x}), y)$ 的期望值
				- 预测损失 $\mathcal{J}(\hat h)$ 可以视为随机变量（训练集的选取视为一个随机过程），那么它的期望值记为 $\bar \mathcal{J}$
				- 定义一个算法 $\mathcal{A}$ ，从训练数据 $\mathcal{D}^{\text {train }}$ 中学习到最优预测函数 $\hat{h}$ 公式 4.22
					-
					  $$
					  \hat{h}=\mathcal{A}\left(\mathcal{D}^{\text {train }}\right)
					  $$
				- 可以得到期望预测损失 expeted prediction loss $\bar \mathcal{J}$ 写作 公式 4.23
					-
					  $$
					  \bar{\mathcal{J}}(\mathcal{A})=\mathbb{E}_{\mathcal{D}^{\text {train }}}[\mathcal{J}(\hat{h})]=\mathbb{E}_{\mathcal{D}^{\text {train }}}\left[\mathcal{J}\left(\mathcal{A}\left(\mathcal{D}^{\text {train }}\right)\right)\right]
					  $$
					- measures the performance of the algorithm $\mathcal{A}$
					- 这玩意儿也是个期望，不能直接算。用交叉验证法 cross-validation 方法来估计 ((603645b8-d154-41bf-b0e3-3c0bd80d3cb2))
			- 4.2.2 Overfitting and Underfitting
				- 修改 公式 4.7 的优化目标，加一个正则项，解决过拟合和欠拟合的问题 公式4.26
					-
					  $$
					  \underset{\boldsymbol{\theta}}{\operatorname{minimise}} J_{\boldsymbol{\lambda}}(\boldsymbol{\theta})+\lambda_{\mathrm{reg}} R(\boldsymbol{\theta})
					  $$
					- $R(\boldsymbol{\theta})$ 惩罚项
					- $\lambda_{\mathrm{reg}}$ 正则化的强度（系数）
			- 4.2.3 Example
				- 预测损失和训练损失不同，正如前面所说
					- {{embed ((6037059e-7f8b-4e5f-99c7-3d4a1e3c591c))}}
		- 4.3 Estimating the Generalisation Performance
			- 4.3.1 Methods for Estimating the Generalisation Performance
			  id:: 603645b8-d154-41bf-b0e3-3c0bd80d3cb2
				- Hold-out Approach
					- Divide $$\mathcal{D}$$ into training set $$\mathcal{D}^{\text{train}}$$ and test or validation set $$\tilde\mathcal{D}$$ (也可以写作 $$\mathcal{D}^{\text{val}}$$ )
					- estimate the prediction loss 公式 4.31
						-
						  id:: 6037159d-bc32-4e58-aed3-830849834ac1
						  $$
						  \hat{\mathcal{J}}(\hat{h} ; \tilde{\mathcal{D}})=\frac{1}{\tilde{n}} \sum_{i=1}^{\tilde{n}} \mathcal{L}\left(\hat{h}\left(\tilde{\boldsymbol{x}}_{i}\right), \tilde{y}_{i}\right)
						  $$
				- Cross-validation
					- Divide $$\mathcal{D}$$  into $K$ subsets $$\mathcal{D}_1,\cdots,\mathcal{D}_K$$
					- 得到训练集和测试集 公式 4.32
						-
						  $$
						  \mathcal{D}_{k}^{\text {train }}=\bigcup_{i \neq k} \mathcal{D}_{i}, \quad \mathcal{D}_{k}^{\text {val }}=\mathcal{D}_{k},
						  $$
					- 计算训练损失 公式 4.33
						-
						  $$
						  \hat{h}_k=\mathcal{A}\left(\mathcal{D}_k^{\text {train }}\right)
						  $$
					- 计算泛化性能 公式 4.34 公式 4.36
						-
						  $$\hat{\mathcal{J}}_k = \hat{\mathcal{J}}(\hat{h}_k ; \mathcal{D}_k^{\text{val}}) = \hat{\mathcal{J}}(\mathcal{A}\left(\mathcal{D}_k^{\text {train }}\right) ; \mathcal{D}_k^{\text{val}}) $$
					- 计算 CV score 公式 4.35 公式 4.37
						-
						  $$\text{CV} = \cfrac{1}{K}\sum_{i=1}^K\hat{\mathcal{J}}_k = \cfrac{1}{K}\sum_{i=1}^K \hat{\mathcal{J}}(\mathcal{A}\left(\mathcal{D}_k^{\text {train }}\right) ; \mathcal{D}_k^{\text{val}}) $$
					- cv score 是对 期望预测损失 $\bar{\mathcal{J}}(\mathcal{A})$ 的估计，记为 $\hat{\bar{\mathcal{J}}}(\mathcal{A})$
					- 公式2.39 计算 cv 的 方差，开个平方，变成 standard error of the cv-score
						-
						  $$
						  \operatorname{Var}(\mathrm{CV}) \approx \frac{1}{K} \operatorname{Var}(\hat{\mathcal{J}}), \quad \operatorname{Var}(\hat{\mathcal{J}}) \approx \frac{1}{K} \sum_{k=1}^{K}\left(\hat{\mathcal{J}}_{k}-\mathrm{CV}\right)^{2}
						  $$
				- leave-one-out cross-validation (LOOCV)，CV的升级版，K取n
			- 4.3.2 Hyperparameter Selection and Performance Evaluation
				- 7 步
				- Two Times Hold-out
					- 1. 分割D，得到Dtest，20%
					- 2. 再2/8分剩下的，得到Dtrain和Dval
					- 3. 固定超参数$\lambda$，运行算法，求最优训练函数（最优内部参数组$$\boldsymbol{\theta}$$）
						-
						  $$
						  \hat{h}_{\boldsymbol{\lambda}}=\mathcal{A}_{\boldsymbol{\lambda}}\left(\mathcal{D}_k^{\text {train }}\right)
						  $$
					- 4. 用Dval评估，求预测损失
						-
						  $$
						  \mathrm{PL}(\boldsymbol{\lambda})=\hat{\mathcal{J}}\left(\hat{h}_{\boldsymbol{\lambda}} ; \mathcal{D}^{\mathrm{val}}\right)
						  $$
						- where
							- {{embed ((6037159d-bc32-4e58-aed3-830849834ac1))}}
						- 得到最优超参数组 $$\hat\boldsymbol{\lambda}$$
							-
							  $$
							  \hat{\boldsymbol{\lambda}}=\underset{\boldsymbol{\lambda}}{\operatorname{argmin}} \mathrm{PL}(\boldsymbol{\lambda})
							  $$
					- 5. 合并Dtrain和Dval，重新优化一次得到新的 $$\boldsymbol{\theta}$$
						-
						  $$
						  \hat{h}=\mathcal{A}_{\hat{\lambda}}\left(\mathcal{D}^{\text {train }} \cup \mathcal{D}^{\mathrm{val}}\right)
						  $$
					- 6. 用Dtest估计预测损失
						-
						  $$
						  \hat{\mathcal{J}}=\hat{\mathcal{J}}\left(\hat{h} ; \mathcal{D}^{\text {test }}\right)
						  $$
					- 7. 合并所有数据集，重新优化一次得到预测函数
						-
						  $$
						  \hat{h}(\boldsymbol{x})=\mathcal{A}_{\hat{\boldsymbol{\lambda}}}(\mathcal{D})
						  $$
				- Cross-validation and Hold-out
					- 1同上
					- 2. k-fold 分割 得到k个Dtrain和k个Dval
					- 3同上，在k个Dtrain上，算出k个最优预测函数（后面平均）
					- 4计算cv score，目标最小化，求得最优参数数组 公式 4.46
						-
						  $$
						  \operatorname{EPL}(\boldsymbol{\lambda})=\mathrm{CV}=\hat{\mathcal{J}}\left(\mathcal{A}_{\lambda}\right)
						  $$
					- 5同上，即用除了Dtest之外的数据计算
					- 6同上
					- 7同上
		- 4.4 Loss Functions in Predictive Modelling
			- 4.4.1 Loss Functions in Regression
				- 公式 4.50 4.51 4.52
					-
					  $$
					  \begin{array}{lll}
					  L(\hat{y}, y) & =\frac{1}{2}(\hat{y}-y)^{2} & \text { (square loss) } \\
					  L(\hat{y}, y) & =|\hat{y}-y| & \\
					  L(\hat{y}, y) & =\left\{\begin{array}{ll}
					  \frac{1}{2}(\hat{y}-y)^{2} & \text { if }|\hat{y}-y|<\delta \\
					  \delta|y-\hat{y}|-\frac{1}{2} \delta^{2} & \text { otherwise }
					  \end{array}\right. & \text { (absolute loss) } \\
					  & & \text { (Huber loss) }
					  \end{array}
					  $$
			- 4.4.2 Loss Functions in Classification
				- Non-diﬀerentiable Loss Functions
					- 损失矩阵 公式 4.53
						-
						  $$
						  \boldsymbol{L}=\left(\begin{array}{cccc}
						  L(1,1) & L(1,2) & \cdots & L(1, K) \\
						  L(2,1) & L(2,2) & \cdots & L(2, K) \\
						  \vdots & \vdots & & \vdots \\
						  L(K, 1) & L(K, 2) & \cdots & L(K, K)
						  \end{array}\right)
						  $$
					- TP，TN，FP，FN
						- ![](https://gitee.com/zhang-weijian-97/pic-go-bed/raw/master/assets/20210225034337.png)
					- misclassiﬁcation or error rate 公式 4.58
						-
						  $$
						  \begin{aligned}
						  \mathcal{J}(h) &=\mathbb{E}_{\boldsymbol{x}, y} L(h(\boldsymbol{x}), y) \\
						  &=\mathbb{E}_{\hat{y}, y} L(\hat{y}, y) \\
						  &=\sum_{i, j} L(i, j) p(i, j) \\
						  &=\sum_{i \neq j} p(i, j) \\
						  &=\mathbb{P}(y \neq \hat{y})
						  \end{aligned}
						  $$
					- loss function that penalises false-positive and false-negative rates 公式 4.68
						-
						  $$
						  \begin{aligned}
						  \mathcal{J}(h) &=\mathbb{E}_{\boldsymbol{x}, y} L(h(\boldsymbol{x}), y) \\
						  &=\mathbb{E}_{\hat{y}, y} L(\hat{y}, y) \\
						  &=\sum_{i, j} L(i, j) p(i, j) \\
						  &=\frac{p(1,-1)}{\mathbb{P}(y=-1)}+\frac{p(-1,1)}{\mathbb{P}(y=1)} \\
						  &=\mathbb{P}(\hat{y}=1 \mid y=-1)+\mathbb{P}(\hat{y}=-1 \mid y=1)
						  \end{aligned}
						  $$
						- where 公式 4.63
							-
							  $$
							  \boldsymbol{L}=\left(\begin{array}{cc}
							  0 & \frac{1}{\mathbb{P}(y=1)} \\
							  \frac{1}{\mathbb{P}(y=-1)} & 0
							  \end{array}\right)
							  $$
					- ROC，TP，FP
				- Diﬀerentiable Loss Functions in Classiﬁcation
					- trick 公式 4.69
						-
						  $$
						  \hat{y}(\boldsymbol{x})=\operatorname{sign}(h(\boldsymbol{x}))
						  $$
					- margin $$yh(x)$$
					- logistic regression
						- 公式
							- ![](https://gitee.com/zhang-weijian-97/pic-go-bed/raw/master/assets/20210225034805.png)
						- 最大化
							- ![](https://gitee.com/zhang-weijian-97/pic-go-bed/raw/master/assets/20210225034818.png){:height 141, :width 472}
- 草稿区
	- 4.1
		- Answer: fraction of variance explained: $\cfrac{5}{6}$
		- Proof:
		- by lecture notes:
			-
			  $$\boldsymbol{x} = \mathbf{a}_{1} z_{1}-4 \mathbf{a}_{2} z_{2}+3 \mathbf{a}_{3} z_{3}+2 \mathbf{a}_{4} z_{4}+\mathbf{m} =  \boldsymbol{W}\boldsymbol{z}+\boldsymbol{m} = \boldsymbol{W}\boldsymbol{z}+\boldsymbol{\mu}+\boldsymbol{\epsilon}$$
		- we got
			-
			  $$\boldsymbol{W} = [\mathbf{a}_{1} ~ ~ -4 \mathbf{a}_{2} ~ ~ 3 \mathbf{a}_{3} ~ ~ 2 \mathbf{a}_{4}]$$
		- because
			-
			  $$
			  \mathbf{m} \text { is a } d \text { -dimensional vector containing constants}
			  $$
		- therefore, $\boldsymbol{\epsilon}$ is discarded
		- therefore, $\sigma = 0$
		- by lecture notes:
			-
			  $$
			  p(\boldsymbol{x})=\mathcal{N}\left(\boldsymbol{x} \mid \boldsymbol{\mu}, \boldsymbol{W} \boldsymbol{W}^{\top}+\sigma^{2} \boldsymbol{I}\right)
			  $$
		- we got the covarience matrix of $\boldsymbol{x}$
			-
			  $$\boldsymbol{\Sigma}=\boldsymbol{W}\boldsymbol{W}^\top+\sigma^{2} \boldsymbol{I} = \boldsymbol{W}\boldsymbol{W}^\top = \left[ \begin{matrix} \boldsymbol{a}_{1}^{2}& 0& 0& 0\\ 0& 16 \boldsymbol{a}^2_2& 0& 0\\ 0& 0& 9\boldsymbol{a}_{3}^{2}& 0\\ 0& 0& 0& 4\boldsymbol{a}_{4}^2 \end{matrix} \right]$$
		- by eigenvalue decomposition
			-
			  $$\boldsymbol{\Sigma} = \boldsymbol{U}\boldsymbol{\Lambda} \boldsymbol{U}^\top = \left[ \begin{matrix} \boldsymbol{a}_{1}& \boldsymbol{a}_2& \boldsymbol{a}_3& \boldsymbol{a}_4 \end{matrix} \right]\left[ \begin{matrix} 1 & 0& 0& 0\\ 0& 16 & 0& 0\\ 0& 0& 9 & 0\\ 0& 0& 0& 4 \end{matrix} \right]\left[ \begin{matrix} \boldsymbol{a}_{1}^\top\\ \boldsymbol{a}_2^\top\\ \boldsymbol{a}_{3}^\top\\\boldsymbol{a}_{4}^\top \end{matrix} \right]$$
		- therefore, we got
			- $\lambda_1 = 16$, $\lambda_2 = 9$, $\lambda_3 = 4$, $\lambda_4 = 1$
		- therfore, by lecture notes, fraction of variance explained is
			-
			  $$\cfrac{\sum_{m=1}^k\lambda_m}{\sum_{m=1}^d\lambda_m} = \cfrac{16 + 9}{16 + 9 + 4+ 1} = \cfrac{5}{6}$$
-
  $$\left[ \begin{matrix} \boldsymbol{a}_{1}^{2}& \boldsymbol{a}^2_2& \boldsymbol{a}^2_3& \boldsymbol{a}^2_4 \end{matrix} \right]$$
-
  $$\left[ \begin{matrix} 1 & 0& 0& 0\\ 0& 16 & 0& 0\\ 0& 0& 9 & 0\\ 0& 0& 0& 4 \end{matrix} \right]$$
-
  $$\left[ \begin{matrix} \boldsymbol{a}_{1}^{2}\\ \boldsymbol{a}^2_2\\ \boldsymbol{a}_{3}^{2}\\\boldsymbol{a}_{4}^2 \end{matrix} \right]$$
- Reference
	- DME lecture notes