---
title: 价值函数近似
---

- [强化学习纲要 第四课 价值函数近似 上](https://www.bilibili.com/video/BV11V411f7bi)
- Valua Function Approximation
- 之前学习的方法解决不了Large scale problem
	- 比如围棋，机械手运动
- 复习，表格法的基础
	- state s and entry V(s)
	- state-action pair (s, a) and entry Q(s, a)
	- ![](https://gitee.com/zhang-weijian-97/pic-go-bed/raw/master/assets/20210301160728.png){:height 171, :width 604}
	- 问题：如果表格非常非常大，没办法保存在内存
	- 解决，用近似，带参数
- 近似函数
	-
	  $$
	  \begin{aligned}
	  \hat{v}(s, \mathbf{w}) & \approx v^{\pi}(s) \\
	  \hat{q}(s, a, \mathbf{w}) & \approx q^{\pi}(s, a) \\
	  \hat{\pi}(a, s, \mathbf{w}) & \approx \pi(a \mid s)
	  \end{aligned}
	  $$
- 近似器
	- 特征的线性组合（可导
	- 神经网络（可导
	- 决策树
	- 近邻
- Oracle
- 值函数估计 （这个似乎是预测问题？）
	- 损失函数
		-
		  $$
		  J(\mathbf{w})=\mathbb{E}_{\pi}\left[\left(v^{\pi}(s)-\hat{v}(s, \mathbf{w})\right)^{2}\right]
		  $$
	- 梯度下降
		-
		  $$
		  \begin{aligned}
		  \Delta \mathbf{w} &=-\frac{1}{2} \alpha \nabla_{\mathbf{w}} J(\mathbf{w}) \\
		  \mathbf{w}_{t+1} &=\mathbf{w}_{t}+\Delta \mathbf{w}
		  \end{aligned}
		  $$
	- 用特征向量来表示状态
		-
		  $$
		  \mathrm{x}(s)=\left(x_{1}(s), \ldots, x_{n}(s)\right)^{T}
		  $$
		- 比如位置，速度，角度等
	- 已知Oracle
		- 表示
			-
			  id:: 603d25b5-9924-421d-971a-107a2ba9470d
			  $$
			  \hat{v}(s, \mathbf{w})=\mathbf{x}(s)^{T} \mathbf{w}=\sum_{j=1}^{n} x_{j}(s) w_{j}
			  $$
		- 目标函数（损失函数）
			-
			  id:: 603d25c0-318b-41d4-8c9d-e6c40cec2f8e
			  $$
			  J(\mathbf{w})=\mathbb{E}_{\pi}\left[\left(v^{\pi}(s)-\mathbf{x}(s)^{T} \mathbf{w}\right)^{2}\right]
			  $$
		- 更新规则
			-
			  id:: 603d25d2-3cf2-40ea-994a-521efe25b6d9
			  $$
			  \Delta \mathbf{w}=\alpha\left(v^{\pi}(s)-\hat{v}(s, \mathbf{w})\right) \mathbf{x}(s)
			  $$
			-
			  $$
			  \text { Update }=\text { Step Size } \times \text { Prediction Error } \times \text { FeatureValue }
			  $$
		- 只有一个全局最优，没有局部最小值
	- Table lookup（线性值函数近似的特殊形式）
		- one hot feature
			-
			  $$
			  \mathrm{x}^{\text {table }}(s)=\left(\mathbf{1}\left(s=s_{1}\right), \ldots, \mathbf{I}\left(s=s_{n}\right)\right)^{T}
			  $$
		- 值函数表示
			-
			  $$
			  \hat{v}(s, \mathbf{w})=\left(\mathbf{1}\left(s=s_{1}\right), \ldots, \mathbf{1}\left(s=s_{n}\right)\right)\left(w_{1}, \ldots, w_{n}\right)^{T}
			  $$
		- 这种方法只能用来表示离散的情况
	- 未知Oracle，Model free Prediction
		- 梯度
			-
			  $$
			  \Delta \mathbf{w}=\alpha\left(v^{\pi}(s)-\hat{v}(s, \mathbf{w})\right) \nabla_{\mathbf{w}} \hat{v}\left(s_{t}, \mathbf{w}\right)
			  $$
		- 现在 $$v^{\pi}(s)$$ 未知，那就替代掉它
		- MC，用回报
			-
			  id:: 603d27ec-30bf-49c3-ace5-b2282314cf36
			  $$
			  \Delta \mathbf{w}=\alpha\left(G_{t}-\hat{v}\left(s_{t}, \mathbf{w}\right)\right) \nabla_{\mathbf{w}} \hat{v}\left(s_{t}, \mathbf{w}\right)
			  $$
			- 回报是无偏差的，但是有噪声
				-
				  $$
				  \mathbb{E}\left[G_{t}\right]=v^{\pi}\left(s_{t}\right)
				  $$
			- 梯度
				-
				  $$
				  \begin{aligned}
				  \Delta \mathbf{w} &=\alpha\left(G_{t}-\hat{v}\left(s_{t}, \mathbf{w}\right)\right) \nabla_{\mathbf{w}} \hat{v}\left(s_{t}, \mathbf{w}\right) \\
				  &=\alpha\left(G_{t}-\hat{v}\left(s_{t}, \mathbf{w}\right)\right) \mathbf{x}\left(s_{t}\right)
				  \end{aligned}
				  $$
		- TD(0)，用TD target
			-
			  id:: 603d294b-4f78-4f9c-aa77-e4e6d62515e9
			  $$
			  \Delta \mathbf{w}=\alpha\left(R_{t+1}+\gamma \hat{v}\left(s_{t+1}, \mathbf{w}\right)-\hat{v}\left(s_{t}, \mathbf{w}\right)\right) \nabla_{\mathbf{w}} \hat{v}\left(s_{t}, \mathbf{w}\right)
			  $$
			- 有偏差
				-
				  $$
				  \mathbb{E}\left[R_{t+1}+\gamma \hat{v}\left(s_{t+1}, \mathbf{w}\right)\right] \neq v^{\pi}\left(s_{t}\right)
				  $$
			- 梯度
				-
				  $$
				  \begin{aligned}
				  \Delta \mathbf{w} &=\alpha\left(R+\gamma \hat{v}\left(s^{\prime}, \mathbf{w}\right)-\hat{v}(s, \mathbf{w})\right) \nabla_{\mathbf{w}} \hat{v}(s, \mathbf{w}) \\
				  &=\alpha\left(R+\gamma \hat{v}\left(s^{\prime}, \mathbf{w}\right)-\hat{v}(s, \mathbf{w})\right) \mathbf{x}(s)
				  \end{aligned}
				  $$
				- semi-gradiant
- 控制
	- 策略评估，估计动作值
	- 策略改进，$\epsilon$-greedy
- 动作值函数估计 （控制问题？）
	-
	  $$
	  \hat{q}(s, a, \mathbf{w}) \approx q^{\pi}(s, a)
	  $$
	- Oracle已知
		- 损失函数，最小化平方误差MSE
			-
			  $$
			  J(\mathbf{w})=\mathbb{E}_{\pi}\left[\left(q^{\pi}(s, a)-\hat{q}(s, a, \mathbf{w})\right)^{2}\right]
			  $$
		- 梯度更新公式（泛用）
			-
			  $$
			  \Delta \mathbf{w}=\alpha\left(q^{\pi}(s, a)-\hat{q}(s, a, \mathbf{w})\right) \nabla_{\mathbf{w}} \hat{q}(s, a, \mathbf{w})
			  $$
		- 特征
			-
			  $$
			  \mathbf{x}(s, a)=\left(x_{1}(s, a), \ldots, x_{n}(s, a)\right)^{T}
			  $$
		- 动作值函数，表示
			-
			  $$
			  \hat{q}(s, a, \mathbf{w})=\mathbf{x}(s, a)^{T} \mathbf{w}=\sum_{j=1}^{n} x_{j}(s, a) w_{j}
			  $$
		- SGD
			-
			  $$
			  \Delta \mathbf{w}=\alpha\left(q^{\pi}(s, a)-\hat{q}(s, a, \mathbf{w})\right) \mathbf{x}(s, a)
			  $$
		- 增量控制算法
	- Oracle未知
		- MC
			-
			  $$
			  \Delta \mathbf{w}=\alpha\left(G_{t}-\hat{q}\left(s_{t}, a_{t}, \mathbf{w}\right)\right) \nabla_{\mathbf{w}} \hat{q}\left(s_{t}, a_{t}, \mathbf{w}\right)
			  $$
		- Sarsa
			-
			  $$
			  \Delta \mathbf{w}=\alpha\left(R_{t+1}+\gamma \hat{q}\left(s_{t+1}, a_{t+1}, \mathbf{w}\right)-\hat{q}\left(s_{t}, a_{t}, \mathbf{w}\right)\right) \nabla_{\mathbf{w}} \hat{q}\left(s_{t}, a_{t}, \mathbf{w}\right)
			  $$
		- Q-leaerning
			-
			  $$
			  \Delta \mathbf{w}=\alpha\left(R_{t+1}+\gamma \max _{a} \hat{q}\left(s_{t+1}, a, \mathbf{w}\right)-\hat{q}\left(s_{t}, a_{t}, \mathbf{w}\right)\right) \nabla_{\mathbf{w}} \hat{q}\left(s_{t}, a_{t}, \mathbf{w}\right)
			  $$
	- 算法，Semi-gradient Sarsa，用于估计动作值函数（控制 Control 任务）
		- ![](https://gitee.com/zhang-weijian-97/pic-go-bed/raw/master/assets/20210301225315.png)
	- 例子，mountain car，略
- 收敛问题 Convergence
	- TD方法（如 Sarsa， Q-learning）没有遵循任何的目标函数，一是用bellman备份，另一个用值函数近似，双重近似引入噪声。TD不稳定，没有收敛保证。
	- Function approximation
	- bootstrapping
	- off-policy training
	- ![](https://gitee.com/zhang-weijian-97/pic-go-bed/raw/master/assets/20210301234322.png)
- Batch RL
	- 上面是一个个，很慢。batch的快
	- 数据 $$\mathcal{D}=\left\{<s_{1}, v_{1}^{\pi}>, \ldots,<s_{t}, v_{T}^{\pi}>\right\}$$
	- 优化目标
		-
		  $$
		  \begin{aligned}
		  \mathbf{w}^{*} &=\underset{\mathbf{w}}{\arg \min } \mathbb{E}_{\mathcal{D}}\left[\left(v^{\pi}-\hat{v}(s, \mathbf{w})^{2}\right]\right.\\
		  &=\underset{\mathbf{w}}{\arg \min } \sum_{t=1}^{T}\left(v_{t}^{\pi}-\hat{v}\left(s_{t}, \mathbf{w}\right)\right)^{2}
		  \end{aligned}
		  $$
	- SGD with Experience Replay
		- 随机采样（比如一个batch 10个数据）
		- 收敛到least square solution
- [强化学习纲要 第四课 价值函数近似 下](https://www.bilibili.com/video/BV1w54y1d7se)
- 上面讲了线性函数来拟合，下面讲非线性函数（神经网络）
- 小复习，线性函数近似
	- 价值函数的表示
		- {{embed ((603d25b5-9924-421d-971a-107a2ba9470d)) }}
	- 目标函数
		- {{embed ((603d25c0-318b-41d4-8c9d-e6c40cec2f8e))}}
	- 梯度更新规则
		- 有Oracle
			- {{embed ((603d25d2-3cf2-40ea-994a-521efe25b6d9))}}
		- 无Oracle
			- MC
				- {{embed ((603d27ec-30bf-49c3-ace5-b2282314cf36))}}
			- TD
				- {{embed ((603d294b-4f78-4f9c-aa77-e4e6d62515e9))}}
- 线性和非线性
	- 线性需要人工挑选很好的特征
- 非线性价值函数
	- Deep Neural Networks，略
	- Convolutional Neural Networks，略
- Deep RL
	- 用深度神经网络来表示 价值函数，策略函数，世界模型
	- 损失函数用SGD
	- 问题
		- 效率
		- 死亡三角
			- 非线性估计
			- 自举
			- off-policy
	- Deep Q-Networks（DQN）
		- 例子，雅达利游戏
			- 输入s，最近的4帧画面
			- 输出Q(s, a)
		- 问题
			- Correlations between samples
				- 不同sample之间非常相似，难以学习
			- Non-stationary targets
				- target里面包含模型参数
		- 解决方法
			- Experience replay
				- replay memory D，储存transition $$\left(s_{t}, a_{t}, r_{t}, s_{t+1}\right)$$
					- ![](https://gitee.com/zhang-weijian-97/pic-go-bed/raw/master/assets/20210302001204.png)
				- 在 D 中随机采样
					- 相关度比较低
				- 再计算TD target
				- 更新梯度
			- Fixed Q targets
				- 2个网络
				- 产生target的网络，有个固定的weight，或
				- 产生target的网络和实际优化的网络有时间差
				- 从 总 数据集采样
				- target value
					-
					  $$
					  r+\gamma \max _{a^{\prime}} \hat{Q}\left(s^{\prime}, a^{\prime}, w^{-}\right)
					  $$
				- 梯度更新
					-
					  $$
					  \Delta \mathbf{w}=\alpha\left(r+\gamma \max _{a^{\prime}} \hat{Q}\left(s^{\prime}, a^{\prime}, \mathbf{w}^{-}\right)-Q(s, a, \mathbf{w})\right) \nabla_{\mathbf{w}} \hat{Q}(s, a, \mathbf{w})
					  $$
				- 理念，Q estimation追赶Q target，导致不稳定。所以每5步固定一步的Q target
	- 改良
		- Double DQN
		- Dueling DQN
		- Prioritized Replay
		- Agent57