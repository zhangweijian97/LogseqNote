---
title: 价值函数近似
---

## [强化学习纲要 第四课 价值函数近似 上](https://www.bilibili.com/video/BV11V411f7bi)
## [强化学习纲要 第四课 价值函数近似 下](https://www.bilibili.com/video/BV1w54y1d7se)
## Valua Function Approximation
## 之前学习的方法解决不了Large scale problem
### 比如围棋，机械手运动
## 复习，表格法的基础
### state s and entry V(s)
### state-action pair (s, a) and entry Q(s, a)
### ![](https://gitee.com/zhang-weijian-97/pic-go-bed/raw/master/assets/20210301160728.png){:height 171, :width 604}
### 问题：如果表格非常非常大，没办法保存在内存
### 解决，用近似，带参数
## 近似函数
###
$$
\begin{aligned}
\hat{v}(s, \mathbf{w}) & \approx v^{\pi}(s) \\
\hat{q}(s, a, \mathbf{w}) & \approx q^{\pi}(s, a) \\
\hat{\pi}(a, s, \mathbf{w}) & \approx \pi(a \mid s)
\end{aligned}
$$
## 近似器
### 特征的线性组合（可导
### 神经网络（可导
### 决策树
### 近邻
## Oracle
## 值函数估计
### 损失函数
####
$$
J(\mathbf{w})=\mathbb{E}_{\pi}\left[\left(v^{\pi}(s)-\hat{v}(s, \mathbf{w})\right)^{2}\right]
$$
### 梯度下降
####
$$
\begin{aligned}
\Delta \mathbf{w} &=-\frac{1}{2} \alpha \nabla_{\mathbf{w}} J(\mathbf{w}) \\
\mathbf{w}_{t+1} &=\mathbf{w}_{t}+\Delta \mathbf{w}
\end{aligned}
$$
### 用特征向量来表示状态
####
$$
\mathrm{x}(s)=\left(x_{1}(s), \ldots, x_{n}(s)\right)^{T}
$$
#### 比如位置，速度，角度等
### 已知Oracle
#### 表示
#####
$$
\hat{v}(s, \mathbf{w})=\mathbf{x}(s)^{T} \mathbf{w}=\sum_{j=1}^{n} x_{j}(s) w_{j}
$$
#### 目标函数（损失函数）
#####
$$
J(\mathbf{w})=\mathbb{E}_{\pi}\left[\left(v^{\pi}(s)-\mathbf{x}(s)^{T} \mathbf{w}\right)^{2}\right]
$$
#### 更新规则
#####
$$
\Delta \mathbf{w}=\alpha\left(v^{\pi}(s)-\hat{v}(s, \mathbf{w})\right) \mathbf{x}(s)
$$
#####
$$
\text { Update }=\text { Step Size } \times \text { Prediction Error } \times \text { FeatureValue }
$$
#### 只有一个全局最优，没有局部最小值
### Table lookup（线性值函数近似的特殊形式）
#### one hot feature
#####
$$
\mathrm{x}^{\text {table }}(s)=\left(\mathbf{1}\left(s=s_{1}\right), \ldots, \mathbf{I}\left(s=s_{n}\right)\right)^{T}
$$
#### 值函数表示
#####
$$
\hat{v}(s, \mathbf{w})=\left(\mathbf{1}\left(s=s_{1}\right), \ldots, \mathbf{1}\left(s=s_{n}\right)\right)\left(w_{1}, \ldots, w_{n}\right)^{T}
$$
#### 这种方法只能用来表示离散的情况
### 未知Oracle，Model free Prediction
#### 梯度
#####
$$
\Delta \mathbf{w}=\alpha\left(v^{\pi}(s)-\hat{v}(s, \mathbf{w})\right) \nabla_{\mathbf{w}} \hat{v}\left(s_{t}, \mathbf{w}\right)
$$
#### 现在 $$v^{\pi}(s)$$ 未知，那就替代掉它
#### MC，用回报
#####
$$
\Delta \mathbf{w}=\alpha\left(G_{t}-\hat{v}\left(s_{t}, \mathbf{w}\right)\right) \nabla_{\mathbf{w}} \hat{v}\left(s_{t}, \mathbf{w}\right)
$$
##### 回报是无偏差的，但是有噪声
######
$$
\mathbb{E}\left[G_{t}\right]=v^{\pi}\left(s_{t}\right)
$$
##### 梯度
######
$$
\begin{aligned}
\Delta \mathbf{w} &=\alpha\left(G_{t}-\hat{v}\left(s_{t}, \mathbf{w}\right)\right) \nabla_{\mathbf{w}} \hat{v}\left(s_{t}, \mathbf{w}\right) \\
&=\alpha\left(G_{t}-\hat{v}\left(s_{t}, \mathbf{w}\right)\right) \mathbf{x}\left(s_{t}\right)
\end{aligned}
$$
#### TD(0)，用TD target
#####
$$
\Delta \mathbf{w}=\alpha\left(R_{t+1}+\gamma \hat{v}\left(s_{t+1}, \mathbf{w}\right)-\hat{v}\left(s_{t}, \mathbf{w}\right)\right) \nabla_{\mathbf{w}} \hat{v}\left(s_{t}, \mathbf{w}\right)
$$
##### 有偏差
######
$$
\mathbb{E}\left[R_{t+1}+\gamma \hat{v}\left(s_{t+1}, \mathbf{w}\right)\right] \neq v^{\pi}\left(s_{t}\right)
$$
##### 梯度
######
$$
\begin{aligned}
\Delta \mathbf{w} &=\alpha\left(R+\gamma \hat{v}\left(s^{\prime}, \mathbf{w}\right)-\hat{v}(s, \mathbf{w})\right) \nabla_{\mathbf{w}} \hat{v}(s, \mathbf{w}) \\
&=\alpha\left(R+\gamma \hat{v}\left(s^{\prime}, \mathbf{w}\right)-\hat{v}(s, \mathbf{w})\right) \mathbf{x}(s)
\end{aligned}
$$
###### semi-gradiant
## 控制
### 策略评估，估计动作值
### 策略改进，$\epsilon$-greedy
## 动作值函数估计
###
$$
\hat{q}(s, a, \mathbf{w}) \approx q^{\pi}(s, a)
$$
### Oracle已知
#### 损失函数，最小化平方误差MSE
#####
$$
J(\mathbf{w})=\mathbb{E}_{\pi}\left[\left(q^{\pi}(s, a)-\hat{q}(s, a, \mathbf{w})\right)^{2}\right]
$$
#### 梯度更新公式（泛用）
#####
$$
\Delta \mathbf{w}=\alpha\left(q^{\pi}(s, a)-\hat{q}(s, a, \mathbf{w})\right) \nabla_{\mathbf{w}} \hat{q}(s, a, \mathbf{w})
$$
#### 特征
#####
$$
\mathbf{x}(s, a)=\left(x_{1}(s, a), \ldots, x_{n}(s, a)\right)^{T}
$$
#### 动作值函数，表示
#####
$$
\hat{q}(s, a, \mathbf{w})=\mathbf{x}(s, a)^{T} \mathbf{w}=\sum_{j=1}^{n} x_{j}(s, a) w_{j}
$$
#### SGD
#####
$$
\Delta \mathbf{w}=\alpha\left(q^{\pi}(s, a)-\hat{q}(s, a, \mathbf{w})\right) \mathbf{x}(s, a)
$$
#### 增量控制算法
### Oracle未知
#### MC
#####
$$
\Delta \mathbf{w}=\alpha\left(G_{t}-\hat{q}\left(s_{t}, a_{t}, \mathbf{w}\right)\right) \nabla_{\mathbf{w}} \hat{q}\left(s_{t}, a_{t}, \mathbf{w}\right)
$$
#### Sarsa
#####
$$
\Delta \mathbf{w}=\alpha\left(R_{t+1}+\gamma \hat{q}\left(s_{t+1}, a_{t+1}, \mathbf{w}\right)-\hat{q}\left(s_{t}, a_{t}, \mathbf{w}\right)\right) \nabla_{\mathbf{w}} \hat{q}\left(s_{t}, a_{t}, \mathbf{w}\right)
$$
#### Q-leaerning
#####
$$
\Delta \mathbf{w}=\alpha\left(R_{t+1}+\gamma \max _{a} \hat{q}\left(s_{t+1}, a, \mathbf{w}\right)-\hat{q}\left(s_{t}, a_{t}, \mathbf{w}\right)\right) \nabla_{\mathbf{w}} \hat{q}\left(s_{t}, a_{t}, \mathbf{w}\right)
$$
### 算法，Semi-gradient Sarsa，用于估计动作值函数（控制任务）
#### ![](https://gitee.com/zhang-weijian-97/pic-go-bed/raw/master/assets/20210301225315.png)
###
