---
title: ERNIE
---

## 临时笔记
### [BERT泛读系列（三）—— ERNIE: Enhanced Language Representation with Informative Entities论文笔记](https://www.jianshu.com/p/5e12e6edbd59)
#### 笔记作者的话
#####
#+BEGIN_QUOTE
主要思路是在BERT的基础上引入了知识（具体来说是实体向量），并且在预训练任务方面提出了Pre-training for Injecting Knowledge
#+END_QUOTE
#### 模型架构 见原文图2
##### 主要的改进是在bert的后段加入了实体向量和经过bert编码后的向量拼接
##### 在输出时多加了实体自编码的任务，从而帮助模型注入实体知识信息。
#### T-Encoder
##### 负责对输入句子（token embedding, segment embedding和positional embedding）进行编码
##### 参考bert即可
#### K-Encoder
##### 这部分是本文重点，是本文创新之处
##### 实体信息的引入
##### 一通操作：
###### 使用了TransE训练实体向量，
###### 再通过多头Attention进行编码（其实可以用更复杂一点的训练方法，应该还有一定的提升空间），
###### 然后通过实体对齐技术，将实体向量加入到对应实体的第一个token编码后的向量上
##### 笔记作者的例子：
######
#+BEGIN_QUOTE
姚明是一个实体，在输入时会被分割成姚、明，最后在这部分引入的姚明这个实体的向量会被加入到要这个字经过bert之后的向量上去
#+END_QUOTE
##### information fusion layer
#### 实体自编码
##### 作者多加入了一个预训练任务 entity auto-encoder(dEA)
##### 类似bert
#### 微调
##### 引入了两个新任务[[Entity Typing]]和[[Relation Classification]]
#### 实验结果
##### 通用任务 相比bert 优势不大
##### 更鲁棒
##### GLUE的评估对ERNIE不是很友好
##### 加复杂度的同时，并没有取得期待的效果
#### 总结
##### 提供了一种很好的实体信息引入思路
###### 我：即前面姚明那个例子。联系实体向量和实体每个token的向量
### [清华等提出ERNIE：知识图谱结合BERT才是“有文化”的语言模型](https://www.linkresearcher.com/theses/040314ac-e50f-4208-a302-75b2bb3d5d2a)
#### 清华大学 张正彦、韩旭、刘知远、孙茂松
#### 华为 诺亚方舟实验室 蒋欣、刘群
#### 知识图谱中的多信息实体（informative entity）可以作为外部知识改善语言表征
#### ERNIE 全名 增强版的语言表征模型
#### 预训练语言表征模型忽略了将知识信息整合到语言理解中
##### 原文图1解释
#### 将外部知识组合到语言表征模型中，我们又会遇到两大主要挑战
##### 结构化的知识编码：对于给定的文本，如何高效地抽取并编码对应的知识图谱事实是非常重要的，这些 KG 事实需要能用于语言表征模型。
##### 异质信息融合：语言表征的预训练过程和知识表征过程有很大的不同，它们会产生两个独立的向量空间。因此，如何设计一个特殊的预训练目标，以融合词汇、句法和知识信息就显得非常重要了。
#### ERNIE 分为抽取知识信息与训练语言模型两大步骤
#### 抽取知识信息
##### 识别文本中的命名实体
##### 与知识图谱中的实体进行匹配
##### 并不直接使用 KG 中基于图的事实，
##### 相反通过知识嵌入算法（例如 TransE）编码 KG 的图结构，并将多信息实体嵌入作为 ERNIE 的输入。
##### 将知识模块的实体表征整合到语义模块的隐藏层中。
###### 我：这应该就是那个姚明的例子
#### 训练语言模型
##### 用带 Mask 的语言模型
##### 预测下一句文本作为预训练目标
##### 新型预训练目标
#### ERNIE 的新目标要求模型同时聚合上下文和知识事实的信息，并同时预测 Token 和实体，从而构建一种知识化的语言表征模型。
#### 针对具体任务进行精调 图3
##### 设计了另一种方法
#### 实验
##### 实体分型
###### 数据集 [[FIGER]] 和 [[Open Entity]]
##### 关系分类
###### 数据集 [[FewRel]] 和 [[TACRED]]
##### [[GLUE]]
##### 模型简化测试
#### 提出知识型聚合器和预训练任务 dEA
#### 在去除远监督的数据的噪声和在有限数据上精调方面的能力都胜过 BERT
#### 未来方向
##### 将知识 注入 [[ELMo]] 等基于特征的预训练模型
##### 将形式各异的结构化知识引入 [[ConceptNet]] 等语言表征模型
###
