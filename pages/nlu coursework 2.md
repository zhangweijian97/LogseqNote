---
title: NLU Coursework 2
---

## 描述
### part 1 补comment，跑baseline
#### question 1 补注释，跑baseline
#### question 2 5个问题，用linux命令
### part 2 扩展baseline
#### Question 3 3个思考问题
#### Question 4 增加层数（命令行），重新训练，分析结果
### part 3 lexical attention model，写代码
#### Question 5 Lexical Model 看论文，实现，重新训练，小数据集调试，分析结果
### part 4 Transformer，add the Multi-Head attention code
#### Question 6 补注释
#### Question 7 实现Multi-Head Attention 重新训练，分析结果
### submission，document，files，不包含个人信息
#### <UUN>.pdf
#### <UUN>.zip
##### lstm.py, train.py, transformer.py and transformer_helper.py
## 队友
### s2068339@ed.ac.uk
## 进度
### question 1 模型跑了一遍，忘记记录时间，估计一两个小时
### 新建overleaf项目
### 申请延期
### q1A
#### [五）通俗易懂理解——BiLSTM](https://zhuanlan.zhihu.com/p/40119926)
### q1D
#### [Incremental Decoding during Inference](https://www.telesens.co/2019/04/21/understanding-incremental-decoding-in-fairseq/#Incremental_Decoding_during_Inference)
#### [Incremental decoding](https://fairseq.readthedocs.io/en/latest/models.html?highlight=incremental#incremental-decoding)
#### [teacher forcing](https://zhuanlan.zhihu.com/p/93030328)
#### [TensorFlow教程翻译 | Neural Machine Translation(seq2seq) Tutorial](https://zhuanlan.zhihu.com/p/33319933)
#### [seq2seq 中的 beam search 算法过程是怎样的](https://www.zhihu.com/question/54356960/answer/138990060)
### q7 implementation
### q7 debuging
###
