---
title: Mar 8th, 2021
---

## TODO
### 毕设 阅读三篇论文
#### [[MSc Project]]
#### {{embed ((603e6e77-6d0a-42b3-ab4c-5dcc56f61e88))}}
#### 先收集一下信息。
##### 年份依次是20，19，20。相当新。
##### 第一篇K-Adapter，
###### 中文笔记
####### [知乎 2020|通过知识适配器向预训练模型中注入知识](https://zhuanlan.zhihu.com/p/106107747)
####### [让预训练模型学习知识：使用多学习器增强知识建模能力](https://www.linkresearcher.com/theses/90d6c22c-c30e-4689-b501-84d28c889df2)
####### [论文阅读笔记 --- 12 K-ADAPTER: Infusing Knowledge into Pre-Trained Models with Adapters](https://blog.csdn.net/smilesooo/article/details/105813705)
###### 代码
####### [官方github库](https://github.com/microsoft/k-adapter)
##### 第二篇 ERNIE
###### 笔记
####### [清华等提出ERNIE：知识图谱结合BERT才是“有文化”的语言模型](https://www.linkresearcher.com/theses/040314ac-e50f-4208-a302-75b2bb3d5d2a)
####### [ERNIE: Enhanced Language Representation... 阅读笔记](https://zhuanlan.zhihu.com/p/70276132)
####### [BERT泛读系列（三）—— ERNIE: Enhanced Language Representation with Informative Entities论文笔记](https://www.jianshu.com/p/5e12e6edbd59)
###### 代码
####### [官方github库](https://github.com/thunlp/ERNIE)
##### 第三篇 KEPLER
###### 笔记
####### 只找到一小段话，
####### 该论文主要通过修改Transformer中的attention机制，通过特殊的mask方法将知识图谱中的相关边考虑到编码过程中，进而增强预训练模型的效果。
####### 首先本文利用CN-DBpedia、HowNet和MedicalKG作为领域内知识图谱，对每一个句子中包含的实体抽取其相关的三元组，这里的三元组被看作是一个短句（首实体，关系，尾实体），与原始的句子合并一起输入给Transformer模型；针对该方法，本文采用基于可见矩阵的mask机制，如下图所示：
####### 从图中可以看出，输入的句子增加了许多三元组构成的短句，在每次编码时针对每一个词，模型通过可视矩阵（0-1变量）来控制该词的视野，使其计算得到的attention分布不会涵盖与其无关的词，进而模拟一个句子树的场景；由于该策略仅仅改动了mask策略，故其可以支持BERT，RoBERTa等一系列模型；该方法最终在8个开放域任务和4个特定领域任务下取得了一定的提升。
