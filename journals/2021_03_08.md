---
title: Mar 8th, 2021
---

## TODO
### 毕设 阅读三篇论文
#### [[MSc Project]]
#### {{embed ((603e6e77-6d0a-42b3-ab4c-5dcc56f61e88))}}
#### 先收集一下信息。
##### 年份依次是20，19，20。相当新。
##### 第一篇K-Adapter，
###### 中文笔记
####### [知乎 2020|通过知识适配器向预训练模型中注入知识](https://zhuanlan.zhihu.com/p/106107747)
####### [让预训练模型学习知识：使用多学习器增强知识建模能力](https://www.linkresearcher.com/theses/90d6c22c-c30e-4689-b501-84d28c889df2)
####### [论文阅读笔记 --- 12 K-ADAPTER: Infusing Knowledge into Pre-Trained Models with Adapters](https://blog.csdn.net/smilesooo/article/details/105813705)
###### 代码
####### [官方github库](https://github.com/microsoft/k-adapter)
##### 第二篇 ERNIE
###### 笔记
####### [清华等提出ERNIE：知识图谱结合BERT才是“有文化”的语言模型](https://www.linkresearcher.com/theses/040314ac-e50f-4208-a302-75b2bb3d5d2a)
####### [ERNIE: Enhanced Language Representation... 阅读笔记](https://zhuanlan.zhihu.com/p/70276132)
####### [BERT泛读系列（三）—— ERNIE: Enhanced Language Representation with Informative Entities论文笔记](https://www.jianshu.com/p/5e12e6edbd59)
###### 代码
####### [官方github库](https://github.com/thunlp/ERNIE)
##### 第三篇 KEPLER
###### 笔记
####### 只找到一小段话：
######## 
#+BEGIN_QUOTE
这篇论文来源于清华和Mila实验室，其主要关注于如何使用BERT增强知识图谱embedding，并帮助增强对应的表示。

　　该论文主要通过添加类似于TransE的预训练机制来增强对应文本的表示，进而增强预训练模型在一些知识图谱有关任务的效果。首先本文基于Wikipedia和Wikidata数据集，将每个entity与对应的维基百科描述相链接，则每个entity均获得其对应的文本描述信息；之后对于每一个三元组——<头实体，关系，尾实体>，本文采用基于BERT对encoder利用entity的描述信息，对每个实体进行编码，如下图所示：

　　从图中可以看出，在通过encoder得到头实体和尾实体对应的表示之后，本文采用类似于TransE的训练方法，即基于头实体和关系预测尾实体。

　　此外本文还采用BERT经典的MLM损失函数，并使用RoBERTa的原始参数进行初始化；最终本文提出的方法在知识图谱补全和若干NLP任务上均带来了增益。
#+END_QUOTE
###### 代码
####### [官方github库](https://github.com/THU-KEG/KEPLER)
##### 第一轮 先过一下中文笔记
###### [[K-Adapter]]
###### [[ERNIE]]
###### [[KEPLER]]
####### 模型
######## encoder
######### 基于transformer
######### 输入
########## sequence of N token
#########
##### 第二轮 三个代码弄下来，尽量跑一下
##### 第三轮 未定 考虑 看原文 或 看代码
### DME Presentation ppt 补充
## [[Richmond 厨房卫生值日]]
##
