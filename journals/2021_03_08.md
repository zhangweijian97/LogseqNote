---
title: Mar 8th, 2021
---

## TODO
### 毕设 阅读三篇论文
#### [[MSc Project]]
#### {{embed ((603e6e77-6d0a-42b3-ab4c-5dcc56f61e88))}}
#### 先收集一下信息。
##### 年份依次是20，19，20。相当新。
##### 第一篇K-Adapter，
###### 中文笔记
####### [知乎 2020|通过知识适配器向预训练模型中注入知识](https://zhuanlan.zhihu.com/p/106107747)
####### [让预训练模型学习知识：使用多学习器增强知识建模能力](https://www.linkresearcher.com/theses/90d6c22c-c30e-4689-b501-84d28c889df2)
####### [论文阅读笔记 --- 12 K-ADAPTER: Infusing Knowledge into Pre-Trained Models with Adapters](https://blog.csdn.net/smilesooo/article/details/105813705)
###### 代码
####### [官方github库](https://github.com/microsoft/k-adapter)
##### 第二篇 ERNIE
###### 笔记
####### [清华等提出ERNIE：知识图谱结合BERT才是“有文化”的语言模型](https://www.linkresearcher.com/theses/040314ac-e50f-4208-a302-75b2bb3d5d2a)
####### [ERNIE: Enhanced Language Representation... 阅读笔记](https://zhuanlan.zhihu.com/p/70276132)
####### [BERT泛读系列（三）—— ERNIE: Enhanced Language Representation with Informative Entities论文笔记](https://www.jianshu.com/p/5e12e6edbd59)
###### 代码
####### [官方github库](https://github.com/thunlp/ERNIE)
##### 第三篇 KEPLER
###### 笔记
####### 只找到一小段话：
######## 
#+BEGIN_QUOTE
这篇论文来源于清华和Mila实验室，其主要关注于如何使用BERT增强知识图谱embedding，并帮助增强对应的表示。

　　该论文主要通过添加类似于TransE的预训练机制来增强对应文本的表示，进而增强预训练模型在一些知识图谱有关任务的效果。首先本文基于Wikipedia和Wikidata数据集，将每个entity与对应的维基百科描述相链接，则每个entity均获得其对应的文本描述信息；之后对于每一个三元组——<头实体，关系，尾实体>，本文采用基于BERT对encoder利用entity的描述信息，对每个实体进行编码，如下图所示：

　　从图中可以看出，在通过encoder得到头实体和尾实体对应的表示之后，本文采用类似于TransE的训练方法，即基于头实体和关系预测尾实体。

　　此外本文还采用BERT经典的MLM损失函数，并使用RoBERTa的原始参数进行初始化；最终本文提出的方法在知识图谱补全和若干NLP任务上均带来了增益。
#+END_QUOTE
###### 代码
####### [官方github库](https://github.com/THU-KEG/KEPLER)
##### 第一轮 先过一下中文笔记
###### [[K-Adapter]]
####### [知乎 2020|通过知识适配器向预训练模型中注入知识](https://zhuanlan.zhihu.com/p/106107747)
######## 动机
######### 研究问题：向大型预训练（语言）模型中注入知识
######### 背景：以无监督方式训练的语言模型很难捕获丰富的知识
######### [[预训练模型]]
########## [[GPT]]、[[BERT]]、[[XLNET]]
######### knowledge-driven
######### standard LM
######### 多任务学习
######### [[终身学习]]（[[continual learning]]）
########## 重新训练
########## 灾难性遗忘 [[catastrophic forgetting]]
######### 耦合的表示（entangled representations）
######### 笔记作者推荐阅读section 2
##########
#+BEGIN_QUOTE
Note：这篇文章中Related Work部分对于向PLM中注入知识这一方向进行了很好的梳理，推荐阅读原文Sec. 2，相关工作的区别主要在于 a) knowledge sources 和 b) training objective；
#+END_QUOTE
######## 本文工作
######### 提出K-Adapter
######### 好处
########## 灵活、简便的向PLM中注入知识的方法
########## 进行持续知识融合
########## 产生解耦的表示
########## 保留了PLM产生的原始表示
########## 可以引入多种知识
######### Adapter
########## Knowledge-specific 模型
########## 作为一个插件加在PLM外部
########## 输入，PLM中间层输出的隐状态
########## 一种知识类型对应于一个Adapter
########## 一个PLM可以连接多个Adapter；
######### 涉及知识类型，数据来源
########## factual knowledge，将Wikipedia文本对齐到Wikidata三元组；
########## linguistic knowledge，对web文本进行依存分析得到；
######### 最终模型
########## 一个Pre-trained language model (PLM) 和两个Adapter
######### [[LAMA]]
######### 对比[[RoBERTa]]
######### 与先前工作的3个不同之处
########## 同时考虑了fact-related和linguistic-related的目标函数（为了同时引入两种类型的知识）；
########## 在注入知识的过程中，原始的PLM参数没有变化；
########## 可以支持持续学习，不同的知识适配器的学习是解耦的（独立的），后续再加入知识不会对已加入的知识产生影响；
######## 模型细节
######### 整体 论文图1
######### Adapter结构 论文图2
########## 结构
########### 每个Adapter模型包含K个adapter层，
############ 每个adapter层包含
############# N 个transformer层；
############# 2 个映射层；
############# 1 个残差连接；
########## 与PLM的连接位置：
########### 将adapter层连接到PLM中不同的transformer层上；
########## 和预训练模型的连接：
########### 当前adapter层的输入：
############ a) transformer层输出的隐藏层，
############ b) 前一个adapter层的输出，这两个表示进行concat；
########### Adapter模型的输出：
############ a) PLM最后一层的隐藏层输出，和
############ b) 最后一个adapter层的输出，进行concat作为最终的输出；
########## 预训练-微调阶段：
########### 不同的Adapter在不同的预训练任务上分别进行训练；
########### 对于不同的下游任务，K-Adapter采用和RoBERTa相同的微调方式；
############ 只使用一种Adapter时，Adapter模型的最终输出作为task-specific层的输入；
############ 使用多种Adapter时，将多个Adapter模型的输出进行concat作为task-specific层的输入；
########## 预训练设置 略
######### Factual Adapter
########## [[Factual Knowledge]] 主要来源于文本中实体之间的关系；
########## 数据集 [[T-REx]]
########## 预训练任务：关系分类
########### 给定context和一对实体，对其间关系标签进行分类；
######### Linguistic Adapter
########## [[Linguistic Knowledge]] 主要来源于文本中词之间的依存关系；
########## 数据集：[[BookCorpus]]
########## 预训练任务：依存关系分类
########### 预测给定句子中每个token在依存分析结果中的father index；
######## 实验
######### 三个下游任务
########## b) [[entity typing]]，
########## c) [[question answering]]；
########## a) [[relation classification]]，
######### QA结果，表格3
######### LAMA query 生成，预测被mask位置的词
###### [[ERNIE]]
####### [BERT泛读系列（三）—— ERNIE: Enhanced Language Representation with Informative Entities论文笔记](https://www.jianshu.com/p/5e12e6edbd59)
########
###### [[KEPLER]]
##### 第二轮 三个代码弄下来，尽量跑一下
##### 第三轮 未定 考虑 看原文 或 看代码
