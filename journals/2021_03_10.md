---
title: Mar 10th, 2021
---

## 晨思
### 说起来，现在才开始写晨思。已经晚上7点38分。之前干嘛去了？下午2点起床，然后和如劼对了下NLU作业。然后就打Arcaea到晚饭。现在刚吃完晚饭。
### 今晚，dme，邓皓元的summary和question。简单搞定掉。
### 还有CLIP
### 还得早睡，明天11点
## TODO
### DONE DME 看别人paper
:PROPERTIES:
:done: 1615409588157
:END:
##### multiple population
##### mixture model？
##### 高斯不行
##### question：what is multiple population and why mixture model with gaussian latent distribution cannot capture the information？
##### 更严格的分布假设 restrictive
##### SPPCA
##### no assumptions made about the distribution of the latent variable.
##### 分布是指数分布家族的任意一个
##### 用Lindsay的方法来估计
##### x的分布
######
$$
P(\mathbf{x})=\int P(\boldsymbol{\theta}) P(\mathbf{x} \mid \boldsymbol{\theta}) d \boldsymbol{\theta}
$$
##### 条件概率
######
$$
P(\mathbf{x} \mid \theta)=\prod_{j} P\left(x_{j} \mid \theta_{j}\right)
$$
######
$$
\log P(x \mid \theta)=\log P_{0}(x)+x \theta-G(\theta)
$$
##### 隐变量分布
######
$$
\boldsymbol{\theta}=\mathbf{a} V+\mathbf{b}
$$
###### 什么是a，V，b
##### what does it mean by n mixture components？does it mean the number of solution of theta？
##### 用 theta和a，或者theta和V，b来表示x的低维表示
##### 估计
###### EM算法
###### 检查收敛
###### 两个criteria，饥饿和临近
##### 选择
###### panalizing complexity
###### minimizing estimate of predicton error
###### Redner
##### 实验
###### low dimenional dataset
###### discrete dataset
###### PCA
###### exponential pca
###### bernoulli gtm
##### 贡献
###### PCA的低维表示是正交的，有限制
###### 用了个隐变量
### DONE DME summary和question
:PROPERTIES:
:done: 1615410779769
:END:
#### [[Mar 10th, 2021]]
#### PCA and PPCA assume the components of the low dimensional representation of x is orthogonal. However, it is possible that the assumption does not hold, for example, the data extracted from multiple populations. To bread this assumption, the paper proposes SPPCA, which represents x by a conditional distribution P(x|theta), where the theta is a latent variable without assumption. The conditional distribution is of the exponential distribution family. It uses the EM algorithm to estimate the theta and uses the maximum likelihood method to estimate the x. The paper compares the SPPCA with PCA, Exponential PCA, and GTM model on 20 newsgroups dataset. The SSPCA achieves a lower log-likelihood on the training and testing data. SSPCA also achieves a higher accuracy when feeding the reduced-dimensional data into an SVM classier.
